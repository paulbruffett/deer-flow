{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d145aa-9671-48a3-9546-577bbd767c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "from typing import Annotated, Literal\n",
    "import json\n",
    "from src.utils.json_utils import repair_json_output\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from mermaid import Mermaid\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3f999-3262-46a4-bdd6-3b463083411f",
   "metadata": {},
   "source": [
    "## LLM Setup\n",
    "Making some changes to simplify the LLM setup - adding a rate limiter so we can use GPT 4.1 and o3 without getting error rate failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f5eddb-173e-47f3-bae6-ee0fd9393f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=4,  \n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83cad40-7ad0-4a4f-bbfd-4d6de5bb0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_yaml_config():\n",
    "    with open(str((Path.cwd().parent / \"conf.yaml\").resolve()), \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        #processed_config = process_dict(config)\n",
    "        return config\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = load_yaml_config()['BASIC_MODEL']['api_key']\n",
    "basic_llm = init_chat_model(\"openai:gpt-4.1-mini\", rate_limiter=rate_limiter)\n",
    "reasoning_llm = init_chat_model(\"openai:gpt-4.1-mini\", rate_limiter=rate_limiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0349bf-93f6-4e30-8c0f-8bbb191104a4",
   "metadata": {},
   "source": [
    "## Configuring State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d425-33bf-4448-959c-304eafad9678",
   "metadata": {},
   "source": [
    "Setting up the Planner and its fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7721975-6517-4e9c-bfdc-fd9e2c506681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class StepType(str, Enum):\n",
    "    RESEARCH = \"research\"\n",
    "    PROCESSING = \"processing\"\n",
    "\n",
    "#used in Plan which is used in State\n",
    "class Step(BaseModel):\n",
    "    need_search: bool = Field(..., description=\"Must be explicitly set for each step\")\n",
    "    title: str\n",
    "    description: str = Field(..., description=\"Specify exactly what data to collect\")\n",
    "    step_type: StepType = Field(..., description=\"Indicates the nature of the step\")\n",
    "    execution_res: Optional[str] = Field(\n",
    "        default=None, description=\"The Step execution result\"\n",
    "    )\n",
    "\n",
    "#used in the State object\n",
    "class Plan(BaseModel):\n",
    "    locale: str = Field(\n",
    "        ..., description=\"e.g. 'en-US' or 'zh-CN', based on the user's language\"\n",
    "    )\n",
    "    has_enough_context: bool\n",
    "    thought: str\n",
    "    title: str\n",
    "    steps: List[Step] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Research & Processing steps to get more context\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"has_enough_context\": False,\n",
    "                    \"thought\": (\n",
    "                        \"To understand the current market trends in AI, we need to gather comprehensive information.\"\n",
    "                    ),\n",
    "                    \"title\": \"AI Market Research Plan\",\n",
    "                    \"steps\": [\n",
    "                        {\n",
    "                            \"need_search\": True,\n",
    "                            \"title\": \"Current AI Market Analysis\",\n",
    "                            \"description\": (\n",
    "                                \"Collect data on market size, growth rates, major players, and investment trends in AI sector.\"\n",
    "                            ),\n",
    "                            \"step_type\": \"research\",\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61536535-d486-4883-9fd1-c74d7e6c8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "#used in State\n",
    "class Resource(BaseModel):\n",
    "    \"\"\"\n",
    "    Resource is a class that represents a resource.\n",
    "    \"\"\"\n",
    "\n",
    "    uri: str = Field(..., description=\"The URI of the resource\")\n",
    "    title: str = Field(..., description=\"The title of the resource\")\n",
    "    description: str | None = Field(\"\", description=\"The description of the resource\")\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"State for the agent system, extends MessagesState with next field.\"\"\"\n",
    "\n",
    "    # Runtime Variables\n",
    "    locale: str = \"en-US\"\n",
    "    research_topic: str = \"\"\n",
    "    observations: list[str] = []\n",
    "    resources: list[Resource] = []\n",
    "    plan_iterations: int = 0\n",
    "    current_plan: Plan | str = None\n",
    "    final_report: str = \"\"\n",
    "    auto_accepted_plan: bool = False\n",
    "    enable_background_investigation: bool = True\n",
    "    background_investigation_results: str = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea13e6-4ed5-47cc-a3b5-30eb18c4e249",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1108e97-1d28-4ede-a550-6a677cff9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2c8b60-c4bf-45fb-97de-065996c0f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_node(\n",
    "    state: State, config: RunnableConfig\n",
    ") -> Command[Literal[\"human_feedback\", \"reporter\"]]:\n",
    "    \"\"\"Planner node that generate the full plan.\"\"\"\n",
    "    logger.info(\"Planner generating full plan\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    plan_iterations = state[\"plan_iterations\"] if state.get(\"plan_iterations\", 0) else 0\n",
    "    messages = apply_prompt_template(\"planner\", state, configurable)\n",
    "\n",
    "    if state.get(\"enable_background_investigation\") and state.get(\n",
    "        \"background_investigation_results\"\n",
    "    ):\n",
    "        messages += [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"background investigation results of user query:\\n\"\n",
    "                    + state[\"background_investigation_results\"]\n",
    "                    + \"\\n\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    plan_llm = reasoning_llm.with_structured_output(\n",
    "            Plan,\n",
    "            method=\"json_mode\",\n",
    "        )\n",
    "\n",
    "    # if the plan iterations is greater than the max plan iterations, return the reporter node\n",
    "    if plan_iterations >= configurable.max_plan_iterations:\n",
    "        return Command(goto=\"reporter\")\n",
    "\n",
    "    full_response = \"\"\n",
    "    response = plan_llm.invoke(messages)\n",
    "    full_response = response.model_dump_json(indent=4, exclude_none=True)\n",
    "\n",
    "    logger.debug(f\"Current state messages: {state['messages']}\")\n",
    "    logger.info(f\"Planner response: {full_response}\")\n",
    "\n",
    "    try:\n",
    "        curr_plan = json.loads(repair_json_output(full_response))\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(\"Planner response is not a valid JSON\")\n",
    "        if plan_iterations > 0:\n",
    "            return Command(goto=\"reporter\")\n",
    "        else:\n",
    "            return Command(goto=\"__end__\")\n",
    "    if curr_plan.get(\"has_enough_context\"):\n",
    "        logger.info(\"Planner response has enough context.\")\n",
    "        new_plan = Plan.model_validate(curr_plan)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=full_response, name=\"planner\")],\n",
    "                \"current_plan\": new_plan,\n",
    "            },\n",
    "            goto=\"reporter\",\n",
    "        )\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [AIMessage(content=full_response, name=\"planner\")],\n",
    "            \"current_plan\": full_response,\n",
    "        },\n",
    "        goto=\"human_feedback\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83187724-5851-423f-99c8-c90cecddbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporter_node(state: State, config: RunnableConfig):\n",
    "    \"\"\"Reporter node that write a final report.\"\"\"\n",
    "    logger.info(\"Reporter write final report\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    current_plan = state.get(\"current_plan\")\n",
    "    input_ = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                f\"# Research Requirements\\n\\n## Task\\n\\n{current_plan.title}\\n\\n## Description\\n\\n{current_plan.thought}\"\n",
    "            )\n",
    "        ],\n",
    "        \"locale\": state.get(\"locale\", \"en-US\"),\n",
    "    }\n",
    "    invoke_messages = apply_prompt_template(\"reporter\", input_, configurable)\n",
    "    observations = state.get(\"observations\", [])\n",
    "\n",
    "    # Add a reminder about the new report format, citation style, and table usage\n",
    "    invoke_messages.append(\n",
    "        HumanMessage(\n",
    "            content=\"IMPORTANT: Structure your report according to the format in the prompt. Remember to include:\\n\\n1. Key Points - A bulleted list of the most important findings\\n2. Overview - A brief introduction to the topic\\n3. Detailed Analysis - Organized into logical sections\\n4. Survey Note (optional) - For more comprehensive reports\\n5. Key Citations - List all references at the end\\n\\nFor citations, DO NOT include inline citations in the text. Instead, place all citations in the 'Key Citations' section at the end using the format: `- [Source Title](URL)`. Include an empty line between each citation for better readability.\\n\\nPRIORITIZE USING MARKDOWN TABLES for data presentation and comparison. Use tables whenever presenting comparative data, statistics, features, or options. Structure tables with clear headers and aligned columns. Example table format:\\n\\n| Feature | Description | Pros | Cons |\\n|---------|-------------|------|------|\\n| Feature 1 | Description 1 | Pros 1 | Cons 1 |\\n| Feature 2 | Description 2 | Pros 2 | Cons 2 |\",\n",
    "            name=\"system\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for observation in observations:\n",
    "        invoke_messages.append(\n",
    "            HumanMessage(\n",
    "                content=f\"Below are some observations for the research task:\\n\\n{observation}\",\n",
    "                name=\"observation\",\n",
    "            )\n",
    "        )\n",
    "    logger.debug(f\"Current invoke messages: {invoke_messages}\")\n",
    "    response = llm.invoke(invoke_messages)\n",
    "    response_content = response.content\n",
    "    logger.info(f\"reporter response: {response_content}\")\n",
    "\n",
    "    return {\"final_report\": response_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0e8779d-81f7-4603-bf6e-c92c03b145c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_node(\n",
    "    state,\n",
    ") -> Command[Literal[\"planner\", \"research_team\", \"reporter\", \"__end__\"]]:\n",
    "    current_plan = state.get(\"current_plan\", \"\")\n",
    "    # check if the plan is auto accepted\n",
    "    auto_accepted_plan = state.get(\"auto_accepted_plan\", False)\n",
    "    if not auto_accepted_plan:\n",
    "        feedback = interrupt(\"Please Review the Plan.\")\n",
    "\n",
    "        # if the feedback is not accepted, return the planner node\n",
    "        if feedback and str(feedback).upper().startswith(\"[EDIT_PLAN]\"):\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(content=feedback, name=\"feedback\"),\n",
    "                    ],\n",
    "                },\n",
    "                goto=\"planner\",\n",
    "            )\n",
    "        elif feedback and str(feedback).upper().startswith(\"[ACCEPTED]\"):\n",
    "            logger.info(\"Plan is accepted by user.\")\n",
    "        else:\n",
    "            raise TypeError(f\"Interrupt value of {feedback} is not supported.\")\n",
    "\n",
    "    # if the plan is accepted, run the following node\n",
    "    plan_iterations = state[\"plan_iterations\"] if state.get(\"plan_iterations\", 0) else 0\n",
    "    goto = \"research_team\"\n",
    "    try:\n",
    "        current_plan = repair_json_output(current_plan)\n",
    "        # increment the plan iterations\n",
    "        plan_iterations += 1\n",
    "        # parse the plan\n",
    "        new_plan = json.loads(current_plan)\n",
    "        if new_plan[\"has_enough_context\"]:\n",
    "            goto = \"reporter\"\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(\"Planner response is not a valid JSON\")\n",
    "        if plan_iterations > 0:\n",
    "            return Command(goto=\"reporter\")\n",
    "        else:\n",
    "            return Command(goto=\"__end__\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"current_plan\": Plan.model_validate(new_plan),\n",
    "            \"plan_iterations\": plan_iterations,\n",
    "            \"locale\": new_plan[\"locale\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf080167-9d3c-49b3-943f-6aba2aaf5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_team_node(state: State):\n",
    "    \"\"\"Research team node that collaborates on tasks.\"\"\"\n",
    "    logger.info(\"Research team is collaborating on tasks.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17209e-f290-4c23-9f7f-2b1ac16f32c3",
   "metadata": {},
   "source": [
    "## Runtime configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24f0f20b-3286-4c94-99b5-a7f1c66dae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "#used in configuration\n",
    "class ReportStyle(enum.Enum):\n",
    "    ACADEMIC = \"academic\"\n",
    "    POPULAR_SCIENCE = \"popular_science\"\n",
    "    NEWS = \"news\"\n",
    "    SOCIAL_MEDIA = \"social_media\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bc13384-ba8f-4233-9f78-6e6bcefde0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields\n",
    "\n",
    "#used in apply_prompt_templates\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields.\"\"\"\n",
    "\n",
    "    resources: list[Resource] = field(\n",
    "        default_factory=list\n",
    "    )  # Resources to be used for the research\n",
    "    max_plan_iterations: int = 1  # Maximum number of plan iterations\n",
    "    max_step_num: int = 3  # Maximum number of steps in a plan\n",
    "    max_search_results: int = 3  # Maximum number of search results\n",
    "    mcp_settings: dict = None  # MCP settings, including dynamic loaded tools\n",
    "    report_style: str = ReportStyle.ACADEMIC.value  # Report style\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196b55a-03cd-4dfe-b4be-e9a3ec6e60fa",
   "metadata": {},
   "source": [
    "### To combine and access State and Config in the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70ce7e08-86b3-47d6-8ab3-8de6965968cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "def apply_prompt_template(\n",
    "    prompt_name: str, state: AgentState, configurable: Configuration = None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Apply template variables to a prompt template and return formatted messages.\n",
    "\n",
    "    Args:\n",
    "        prompt_name: Name of the prompt template to use\n",
    "        state: Current agent state containing variables to substitute\n",
    "\n",
    "    Returns:\n",
    "        List of messages with the system prompt as the first message\n",
    "    \"\"\"\n",
    "    # Convert state to dict for template rendering\n",
    "    state_vars = {\n",
    "        \"CURRENT_TIME\": datetime.now().strftime(\"%a %b %d %Y %H:%M:%S %z\"),\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "    # Add configurable variables\n",
    "    if configurable:\n",
    "        state_vars.update(dataclasses.asdict(configurable))\n",
    "\n",
    "    try:\n",
    "        template = env.get_template(f\"templates/{prompt_name}.md\")\n",
    "        system_prompt = template.render(**state_vars)\n",
    "        return [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying template {prompt_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16ee60-c7f5-448f-9aed-44d76526a9bd",
   "metadata": {},
   "source": [
    "## Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c76f93e9-5a4d-4f97-b3ea-464796427dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "\n",
    "# Initialize Jinja2 environment\n",
    "env = Environment(\n",
    "    loader=FileSystemLoader(os.path.dirname('.')),\n",
    "    autoescape=select_autoescape(),\n",
    "    trim_blocks=True,\n",
    "    lstrip_blocks=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a8a852-f3a5-42a6-86d8-e4a3232d3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "graph_builder.add_node(\"planner\",planner_node)\n",
    "graph_builder.add_node(\"reporter\",reporter_node)\n",
    "graph_builder.add_node(\"human_feedback\",human_feedback_node)\n",
    "graph_builder.add_node(\"research_team\",research_team_node)\n",
    "graph_builder.add_edge(START, \"planner\")\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f736c6a3-cf94-4994-9e10-b8170dfa4935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-84882746-8fc3-4a4e-946d-cebd41027fbb\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = '%%{init: {\"flowchart\": {\"curve\": \"linear\"}}}%%\\ngraph TD;\\n\t__start__([<p>__start__</p>]):::first\\n\tplanner(planner)\\n\treporter(reporter)\\n\thuman_feedback(human_feedback)\\n\tresearch_team(research_team)\\n\t__end__([<p>__end__</p>]):::last\\n\t__start__ --> planner;\\n\thuman_feedback -.-> __end__;\\n\thuman_feedback -.-> planner;\\n\thuman_feedback -.-> reporter;\\n\thuman_feedback -.-> research_team;\\n\tplanner -.-> human_feedback;\\n\tplanner -.-> reporter;\\n\treporter --> __end__;\\n\tresearch_team --> __end__;\\n\tclassDef default fill:#f2f0ff,line-height:1.2\\n\tclassDef first fill-opacity:0\\n\tclassDef last fill:#bfb6fc\\n';\n",
       "            const element = document.querySelector('.mermaid-84882746-8fc3-4a4e-946d-cebd41027fbb');\n",
       "            const { svg } = await mermaid.render('graphDiv-84882746-8fc3-4a4e-946d-cebd41027fbb', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x11a012900>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mermaid(graph.get_graph(xray=True).draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f771e1c-a9d0-4058-8abb-3b65becdca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"node execution\")\n",
    "from datetime import datetime\n",
    "import dataclasses\n",
    "\n",
    "question = \"\"\"What is the outlook for the cloud computing industry both in the west and emerging markets or regions?\n",
    "What are some of the drivers of its growth and possible challenges or constraints?  What does the competitive landscape look like for leading\n",
    "providers of cloud services?\"\"\"\n",
    "\n",
    "initial_state = {\n",
    "        #Setting up State\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
    "        \"auto_accepted_plan\": True,\n",
    "        \"enable_background_investigation\": True,\n",
    "    }\n",
    "\n",
    "config = {\n",
    "    #runtime configuration or variables\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"default\",\n",
    "            \"max_plan_iterations\": 1,\n",
    "            \"max_step_num\": 3,\n",
    "            \"mcp_settings\": {\n",
    "                \"servers\": {\n",
    "                    \"mcp-github-trending\": {\n",
    "                        \"transport\": \"stdio\",\n",
    "                        \"command\": \"uvx\",\n",
    "                        \"args\": [\"mcp-github-trending\"],\n",
    "                        \"enabled_tools\": [\"get_github_trending_repositories\"],\n",
    "                        \"add_to_agents\": [\"researcher\"],\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"recursion_limit\": 100,\n",
    "    }\n",
    "\n",
    "last_message_cnt = 0\n",
    "events = list(graph.stream(input=initial_state, config=config, stream_mode=\"values\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79d89f43-7c71-4e23-9f13-6a135bdf0118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner\n",
      "\n",
      "{\n",
      "    \"locale\": \"en-US\",\n",
      "    \"has_enough_context\": false,\n",
      "    \"thought\": \"The user wants a comprehensive outlook on the cloud computing industry covering both western and emerging markets. They want detailed insights into growth drivers, challenges, constraints, and the competitive landscape among leading cloud providers. Given the multifaceted nature of the query covering geographic segmentation, market drivers, risks, and competition, and the need for abundant, up-to-date data points and analysis, the current context is insufficient and requires structured research. A well-rounded analysis should include historical and current market data, regional comparisons, detailed growth drivers and constraints, and a thorough competitive analysis involving key players and market trends.\",\n",
      "    \"title\": \"Comprehensive Outlook and Competitive Analysis of the Cloud Computing Industry in Western and Emerging Markets\",\n",
      "    \"steps\": [\n",
      "        {\n",
      "            \"need_search\": true,\n",
      "            \"title\": \"Market Overview and Growth Drivers of Cloud Computing in Western and Emerging Markets\",\n",
      "            \"description\": \"Collect detailed data on the current market size, growth rates, and future forecasts for cloud computing in major western regions (e.g., North America, Europe) and key emerging markets (e.g., Asia-Pacific, Latin America, Africa). Identify and analyze key drivers of growth such as digital transformation, adoption trends, technological advancements, regulatory impacts, infrastructure development, and economic factors. Include multiple perspectives, such as industry reports, expert analyses, and market research data from recent credible sources.\",\n",
      "            \"step_type\": \"research\"\n",
      "        },\n",
      "        {\n",
      "            \"need_search\": true,\n",
      "            \"title\": \"Challenges and Constraints Facing Cloud Computing Industry Globally and Regionally\",\n",
      "            \"description\": \"Gather comprehensive information on the key challenges and constraints cloud computing faces in both western and emerging markets. Cover technical challenges, regulatory and compliance issues, data privacy and sovereignty concerns, infrastructure limitations, cost factors, talent shortages, geopolitical and economic risks, and other barriers to growth. Include both mainstream and alternative viewpoints, referencing industry expert commentary, governmental and regulatory publications, and case studies where applicable.\",\n",
      "            \"step_type\": \"research\"\n",
      "        },\n",
      "        {\n",
      "            \"need_search\": true,\n",
      "            \"title\": \"Competitive Landscape Analysis of Leading Cloud Service Providers\",\n",
      "            \"description\": \"Compile in-depth competitive analysis covering major cloud service providers like Amazon Web Services, Microsoft Azure, Google Cloud, Alibaba Cloud, and others active in western and emerging markets. Include market share data, service offerings, regional presence, strategic initiatives, partnerships, pricing strategies, and innovation capabilities. Analyze how competition varies across regions and platforms and what this implies for future market positioning. Incorporate insights from financial reports, market intelligence, industry analyst reports, and news on recent developments.\",\n",
      "            \"step_type\": \"research\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "events[1]['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988706af-4bf9-4ce6-b517-9a685bb1a91d",
   "metadata": {},
   "source": [
    "### Examining the state of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16284d51-6325-432a-a18c-c43a77066d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auto_accepted_plan',\n",
       " 'current_plan',\n",
       " 'enable_background_investigation',\n",
       " 'locale',\n",
       " 'messages',\n",
       " 'plan_iterations'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "{k for k, v in snapshot.values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e341446f-0b5f-4c71-910f-d1b05e6f1249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"---\\nCURRENT_TIME: Fri Jun 20 2025 16:54:48 \\n---\\n\\nYou are a professional Deep Researcher. Study and plan information gathering tasks using a team of specialized agents to collect comprehensive data.\\n\\n# Details\\n\\nYou are tasked with orchestrating a research team to gather comprehensive information for a given requirement. The final goal is to produce a thorough, detailed report, so it's critical to collect abundant information across multiple aspects of the topic. Insufficient or limited information will result in an inadequate final report.\\n\\nAs a Deep Researcher, you can breakdown the major subject into sub-topics and expand the depth breadth of user's initial question if applicable.\\n\\n## Information Quantity and Quality Standards\\n\\nThe successful research plan must meet these standards:\\n\\n1. **Comprehensive Coverage**:\\n   - Information must cover ALL aspects of the topic\\n   - Multiple perspectives must be represented\\n   - Both mainstream and alternative viewpoints should be included\\n\\n2. **\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#understanding apply_prompt_template output\n",
    "apply_prompt_template(\"planner\", snapshot.values, Configuration.from_runnable_config(config))[0]['content'][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e4dbe84-f926-4ad9-9540-8ede8c6dcc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Messages:  2 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('research_team',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('human_feedback',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  1 Next:  ('planner',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  0 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#graph steps and message collection\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a02bd5-6526-4ed7-88d6-610461a50d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
