{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d145aa-9671-48a3-9546-577bbd767c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import yaml\n",
    "from typing import Dict, Any\n",
    "from typing import Annotated, Literal\n",
    "import json\n",
    "from src.utils.json_utils import repair_json_output\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from mermaid import Mermaid\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import asyncio\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a3f999-3262-46a4-bdd6-3b463083411f",
   "metadata": {},
   "source": [
    "## LLM Setup\n",
    "Making some changes to simplify the LLM setup - adding a rate limiter so we can use GPT 4.1 and o3 without getting error rate failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8f5eddb-173e-47f3-bae6-ee0fd9393f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=4,  \n",
    "    check_every_n_seconds=0.1,\n",
    "    max_bucket_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e83cad40-7ad0-4a4f-bbfd-4d6de5bb0881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_yaml_config():\n",
    "    with open(str((Path.cwd().parent / \"conf.yaml\").resolve()), \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "        #processed_config = process_dict(config)\n",
    "        return config\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = load_yaml_config()['BASIC_MODEL']['api_key']\n",
    "basic_llm = init_chat_model(\"openai:gpt-4.1-mini\", rate_limiter=rate_limiter)\n",
    "reasoning_llm = init_chat_model(\"openai:gpt-4.1-mini\", rate_limiter=rate_limiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0349bf-93f6-4e30-8c0f-8bbb191104a4",
   "metadata": {},
   "source": [
    "## Configuring State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d425-33bf-4448-959c-304eafad9678",
   "metadata": {},
   "source": [
    "Setting up the Planner and its fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7721975-6517-4e9c-bfdc-fd9e2c506681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class StepType(str, Enum):\n",
    "    RESEARCH = \"research\"\n",
    "    PROCESSING = \"processing\"\n",
    "\n",
    "#used in Plan which is used in State\n",
    "class Step(BaseModel):\n",
    "    need_search: bool = Field(..., description=\"Must be explicitly set for each step\")\n",
    "    title: str\n",
    "    description: str = Field(..., description=\"Specify exactly what data to collect\")\n",
    "    step_type: StepType = Field(..., description=\"Indicates the nature of the step\")\n",
    "    execution_res: Optional[str] = Field(\n",
    "        default=None, description=\"The Step execution result\"\n",
    "    )\n",
    "\n",
    "#used in the State object\n",
    "class Plan(BaseModel):\n",
    "    locale: str = Field(\n",
    "        ..., description=\"e.g. 'en-US' or 'zh-CN', based on the user's language\"\n",
    "    )\n",
    "    has_enough_context: bool\n",
    "    thought: str\n",
    "    title: str\n",
    "    steps: List[Step] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Research & Processing steps to get more context\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        json_schema_extra = {\n",
    "            \"examples\": [\n",
    "                {\n",
    "                    \"has_enough_context\": False,\n",
    "                    \"thought\": (\n",
    "                        \"To understand the current market trends in AI, we need to gather comprehensive information.\"\n",
    "                    ),\n",
    "                    \"title\": \"AI Market Research Plan\",\n",
    "                    \"steps\": [\n",
    "                        {\n",
    "                            \"need_search\": True,\n",
    "                            \"title\": \"Current AI Market Analysis\",\n",
    "                            \"description\": (\n",
    "                                \"Collect data on market size, growth rates, major players, and investment trends in AI sector.\"\n",
    "                            ),\n",
    "                            \"step_type\": \"research\",\n",
    "                        }\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61536535-d486-4883-9fd1-c74d7e6c8e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "#used in State\n",
    "class Resource(BaseModel):\n",
    "    \"\"\"\n",
    "    Resource is a class that represents a resource.\n",
    "    \"\"\"\n",
    "\n",
    "    uri: str = Field(..., description=\"The URI of the resource\")\n",
    "    title: str = Field(..., description=\"The title of the resource\")\n",
    "    description: str | None = Field(\"\", description=\"The description of the resource\")\n",
    "\n",
    "class State(MessagesState):\n",
    "    \"\"\"State for the agent system, extends MessagesState with next field.\"\"\"\n",
    "\n",
    "    # Runtime Variables\n",
    "    locale: str = \"en-US\"\n",
    "    research_topic: str = \"\"\n",
    "    observations: list[str] = []\n",
    "    resources: list[Resource] = []\n",
    "    plan_iterations: int = 0\n",
    "    current_plan: Plan | str = None\n",
    "    final_report: str = \"\"\n",
    "    auto_accepted_plan: bool = False\n",
    "    enable_background_investigation: bool = True\n",
    "    background_investigation_results: str = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bea13e6-4ed5-47cc-a3b5-30eb18c4e249",
   "metadata": {},
   "source": [
    "## Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1108e97-1d28-4ede-a550-6a677cff9b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.types import Command, interrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b2c8b60-c4bf-45fb-97de-065996c0f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def planner_node(\n",
    "    state: State, config: RunnableConfig\n",
    ") -> Command[Literal[\"human_feedback\", \"reporter\"]]:\n",
    "    \"\"\"Planner node that generate the full plan.\"\"\"\n",
    "    logger.info(\"Planner generating full plan\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    plan_iterations = state[\"plan_iterations\"] if state.get(\"plan_iterations\", 0) else 0\n",
    "    messages = apply_prompt_template(\"planner\", state, configurable)\n",
    "\n",
    "    if state.get(\"enable_background_investigation\") and state.get(\n",
    "        \"background_investigation_results\"\n",
    "    ):\n",
    "        messages += [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": (\n",
    "                    \"background investigation results of user query:\\n\"\n",
    "                    + state[\"background_investigation_results\"]\n",
    "                    + \"\\n\"\n",
    "                ),\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    plan_llm = reasoning_llm.with_structured_output(\n",
    "            Plan,\n",
    "            method=\"json_mode\",\n",
    "        )\n",
    "\n",
    "    # if the plan iterations is greater than the max plan iterations, return the reporter node\n",
    "    if plan_iterations >= configurable.max_plan_iterations:\n",
    "        return Command(goto=\"reporter\")\n",
    "\n",
    "    full_response = \"\"\n",
    "    response = plan_llm.invoke(messages)\n",
    "    full_response = response.model_dump_json(indent=4, exclude_none=True)\n",
    "\n",
    "    logger.debug(f\"Current state messages: {state['messages']}\")\n",
    "    logger.info(f\"Planner response: {full_response}\")\n",
    "\n",
    "    try:\n",
    "        curr_plan = json.loads(repair_json_output(full_response))\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(\"Planner response is not a valid JSON\")\n",
    "        if plan_iterations > 0:\n",
    "            return Command(goto=\"reporter\")\n",
    "        else:\n",
    "            return Command(goto=\"__end__\")\n",
    "    if curr_plan.get(\"has_enough_context\"):\n",
    "        logger.info(\"Planner response has enough context.\")\n",
    "        new_plan = Plan.model_validate(curr_plan)\n",
    "        return Command(\n",
    "            update={\n",
    "                \"messages\": [AIMessage(content=full_response, name=\"planner\")],\n",
    "                \"current_plan\": new_plan,\n",
    "            },\n",
    "            goto=\"reporter\",\n",
    "        )\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [AIMessage(content=full_response, name=\"planner\")],\n",
    "            \"current_plan\": full_response,\n",
    "        },\n",
    "        goto=\"human_feedback\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83187724-5851-423f-99c8-c90cecddbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporter_node(state: State, config: RunnableConfig):\n",
    "    \"\"\"Reporter node that write a final report.\"\"\"\n",
    "    logger.info(\"Reporter write final report\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    current_plan = state.get(\"current_plan\")\n",
    "    input_ = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                f\"# Research Requirements\\n\\n## Task\\n\\n{current_plan.title}\\n\\n## Description\\n\\n{current_plan.thought}\"\n",
    "            )\n",
    "        ],\n",
    "        \"locale\": state.get(\"locale\", \"en-US\"),\n",
    "    }\n",
    "    invoke_messages = apply_prompt_template(\"reporter\", input_, configurable)\n",
    "    observations = state.get(\"observations\", [])\n",
    "\n",
    "    # Add a reminder about the new report format, citation style, and table usage\n",
    "    invoke_messages.append(\n",
    "        HumanMessage(\n",
    "            content=\"IMPORTANT: Structure your report according to the format in the prompt. Remember to include:\\n\\n1. Key Points - A bulleted list of the most important findings\\n2. Overview - A brief introduction to the topic\\n3. Detailed Analysis - Organized into logical sections\\n4. Survey Note (optional) - For more comprehensive reports\\n5. Key Citations - List all references at the end\\n\\nFor citations, DO NOT include inline citations in the text. Instead, place all citations in the 'Key Citations' section at the end using the format: `- [Source Title](URL)`. Include an empty line between each citation for better readability.\\n\\nPRIORITIZE USING MARKDOWN TABLES for data presentation and comparison. Use tables whenever presenting comparative data, statistics, features, or options. Structure tables with clear headers and aligned columns. Example table format:\\n\\n| Feature | Description | Pros | Cons |\\n|---------|-------------|------|------|\\n| Feature 1 | Description 1 | Pros 1 | Cons 1 |\\n| Feature 2 | Description 2 | Pros 2 | Cons 2 |\",\n",
    "            name=\"system\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for observation in observations:\n",
    "        invoke_messages.append(\n",
    "            HumanMessage(\n",
    "                content=f\"Below are some observations for the research task:\\n\\n{observation}\",\n",
    "                name=\"observation\",\n",
    "            )\n",
    "        )\n",
    "    logger.debug(f\"Current invoke messages: {invoke_messages}\")\n",
    "    response = basic_llm.invoke(invoke_messages)\n",
    "    response_content = response.content\n",
    "    logger.info(f\"reporter response: {response_content}\")\n",
    "\n",
    "    return {\"final_report\": response_content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc3bc05a-01e9-4d95-9311-6ab34326ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_investigation_node(state: State, config: RunnableConfig):\n",
    "    logger.info(\"background investigation node is running.\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    query = state.get(\"research_topic\")\n",
    "    background_investigation_results = None\n",
    "    searched_content = LoggedTavilySearch(\n",
    "            max_results=configurable.max_search_results\n",
    "        ).invoke(query)\n",
    "    if isinstance(searched_content, list):\n",
    "            background_investigation_results = [\n",
    "                f\"## {elem['title']}\\n\\n{elem['content']}\" for elem in searched_content\n",
    "            ]\n",
    "            return {\n",
    "                \"background_investigation_results\": \"\\n\\n\".join(\n",
    "                    background_investigation_results\n",
    "                )\n",
    "            }\n",
    "    else:\n",
    "            logger.error(\n",
    "                f\"Tavily search returned malformed response: {searched_content}\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"background_investigation_results\": json.dumps(\n",
    "            background_investigation_results, ensure_ascii=False\n",
    "        )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64087bcb-4b78-4d8a-b5a1-01dc6f20ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def coder_node(\n",
    "    state: State, config: RunnableConfig\n",
    ") -> Command[Literal[\"research_team\"]]:\n",
    "    \"\"\"Coder node that do code analysis.\"\"\"\n",
    "    logger.info(\"Coder node is coding.\")\n",
    "    return await _setup_and_execute_agent_step(\n",
    "        state,\n",
    "        config,\n",
    "        \"coder\",\n",
    "        [python_repl_tool],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bbcea9d-74c6-45d9-a93b-bd84b1f30196",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def researcher_node(\n",
    "    state: State, config: RunnableConfig\n",
    ") -> Command[Literal[\"research_team\"]]:\n",
    "    \"\"\"Researcher node that do research\"\"\"\n",
    "    logger.info(\"Researcher node is researching.\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    tools = [get_web_search_tool(configurable.max_search_results), crawl_tool]\n",
    "    retriever_tool = get_retriever_tool(state.get(\"resources\", []))\n",
    "    if retriever_tool:\n",
    "        tools.insert(0, retriever_tool)\n",
    "    logger.info(f\"Researcher tools: {tools}\")\n",
    "    return await _setup_and_execute_agent_step(\n",
    "        state,\n",
    "        config,\n",
    "        \"researcher\",\n",
    "        tools,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42add38-a94b-4ca9-9952-5dab9d348880",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _setup_and_execute_agent_step(\n",
    "    state: State,\n",
    "    config: RunnableConfig,\n",
    "    agent_type: str,\n",
    "    default_tools: list,\n",
    ") -> Command[Literal[\"research_team\"]]:\n",
    "    \"\"\"Helper function to set up an agent with appropriate tools and execute a step.\n",
    "\n",
    "    This function handles the common logic for both researcher_node and coder_node:\n",
    "    1. Configures MCP servers and tools based on agent type\n",
    "    2. Creates an agent with the appropriate tools or uses the default agent\n",
    "    3. Executes the agent on the current step\n",
    "\n",
    "    Args:\n",
    "        state: The current state\n",
    "        config: The runnable config\n",
    "        agent_type: The type of agent (\"researcher\" or \"coder\")\n",
    "        default_tools: The default tools to add to the agent\n",
    "\n",
    "    Returns:\n",
    "        Command to update state and go to research_team\n",
    "    \"\"\"\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    mcp_servers = {}\n",
    "    enabled_tools = {}\n",
    "\n",
    "    # Extract MCP server configuration for this agent type\n",
    "    if configurable.mcp_settings:\n",
    "        for server_name, server_config in configurable.mcp_settings[\"servers\"].items():\n",
    "            if (\n",
    "                server_config[\"enabled_tools\"]\n",
    "                and agent_type in server_config[\"add_to_agents\"]\n",
    "            ):\n",
    "                mcp_servers[server_name] = {\n",
    "                    k: v\n",
    "                    for k, v in server_config.items()\n",
    "                    if k in (\"transport\", \"command\", \"args\", \"url\", \"env\")\n",
    "                }\n",
    "                for tool_name in server_config[\"enabled_tools\"]:\n",
    "                    enabled_tools[tool_name] = server_name\n",
    "\n",
    "    # Create and execute agent with MCP tools if available\n",
    "    if mcp_servers:\n",
    "        async with MultiServerMCPClient(mcp_servers) as client:\n",
    "            loaded_tools = default_tools[:]\n",
    "            for tool in client.get_tools():\n",
    "                if tool.name in enabled_tools:\n",
    "                    tool.description = (\n",
    "                        f\"Powered by '{enabled_tools[tool.name]}'.\\n{tool.description}\"\n",
    "                    )\n",
    "                    loaded_tools.append(tool)\n",
    "            agent = create_agent(agent_type, agent_type, loaded_tools, agent_type)\n",
    "            return await _execute_agent_step(state, agent, agent_type)\n",
    "    else:\n",
    "        # Use default tools if no MCP servers are configured\n",
    "        agent = create_agent(agent_type, agent_type, default_tools, agent_type)\n",
    "        return await _execute_agent_step(state, agent, agent_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9db8562e-a0c5-4f5c-aa42-c06994860238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinator_node(\n",
    "    state: State, config: RunnableConfig\n",
    ") -> Command[Literal[\"planner\", \"background_investigator\", \"__end__\"]]:\n",
    "    \"\"\"Coordinator node that communicate with customers.\"\"\"\n",
    "    logger.info(\"Coordinator talking.\")\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    messages = apply_prompt_template(\"coordinator\", state)\n",
    "    response = (basic_llm.bind_tools([handoff_to_planner]).invoke(messages))\n",
    "    logger.debug(f\"Current state messages: {state['messages']}\")\n",
    "\n",
    "    goto = \"__end__\"\n",
    "    locale = state.get(\"locale\", \"en-US\")  # Default locale if not specified\n",
    "    research_topic = state.get(\"research_topic\", \"\")\n",
    "\n",
    "    if len(response.tool_calls) > 0:\n",
    "        goto = \"planner\"\n",
    "        if state.get(\"enable_background_investigation\"):\n",
    "            # if the search_before_planning is True, add the web search tool to the planner agent\n",
    "            goto = \"background_investigator\"\n",
    "        try:\n",
    "            for tool_call in response.tool_calls:\n",
    "                if tool_call.get(\"name\", \"\") != \"handoff_to_planner\":\n",
    "                    continue\n",
    "                if tool_call.get(\"args\", {}).get(\"locale\") and tool_call.get(\n",
    "                    \"args\", {}\n",
    "                ).get(\"research_topic\"):\n",
    "                    locale = tool_call.get(\"args\", {}).get(\"locale\")\n",
    "                    research_topic = tool_call.get(\"args\", {}).get(\"research_topic\")\n",
    "                    break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing tool calls: {e}\")\n",
    "    else:\n",
    "        logger.warning(\n",
    "            \"Coordinator response contains no tool calls. Terminating workflow execution.\"\n",
    "        )\n",
    "        logger.debug(f\"Coordinator response: {response}\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"locale\": locale,\n",
    "            \"research_topic\": research_topic,\n",
    "            \"resources\": configurable.resources,\n",
    "        },\n",
    "        goto=goto,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0e8779d-81f7-4603-bf6e-c92c03b145c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_feedback_node(\n",
    "    state,\n",
    ") -> Command[Literal[\"planner\", \"research_team\", \"reporter\", \"__end__\"]]:\n",
    "    current_plan = state.get(\"current_plan\", \"\")\n",
    "    # check if the plan is auto accepted\n",
    "    auto_accepted_plan = state.get(\"auto_accepted_plan\", False)\n",
    "    if not auto_accepted_plan:\n",
    "        feedback = interrupt(\"Please Review the Plan.\")\n",
    "\n",
    "        # if the feedback is not accepted, return the planner node\n",
    "        if feedback and str(feedback).upper().startswith(\"[EDIT_PLAN]\"):\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"messages\": [\n",
    "                        HumanMessage(content=feedback, name=\"feedback\"),\n",
    "                    ],\n",
    "                },\n",
    "                goto=\"planner\",\n",
    "            )\n",
    "        elif feedback and str(feedback).upper().startswith(\"[ACCEPTED]\"):\n",
    "            logger.info(\"Plan is accepted by user.\")\n",
    "        else:\n",
    "            raise TypeError(f\"Interrupt value of {feedback} is not supported.\")\n",
    "\n",
    "    # if the plan is accepted, run the following node\n",
    "    plan_iterations = state[\"plan_iterations\"] if state.get(\"plan_iterations\", 0) else 0\n",
    "    goto = \"research_team\"\n",
    "    try:\n",
    "        current_plan = repair_json_output(current_plan)\n",
    "        # increment the plan iterations\n",
    "        plan_iterations += 1\n",
    "        # parse the plan\n",
    "        new_plan = json.loads(current_plan)\n",
    "        if new_plan[\"has_enough_context\"]:\n",
    "            goto = \"reporter\"\n",
    "    except json.JSONDecodeError:\n",
    "        logger.warning(\"Planner response is not a valid JSON\")\n",
    "        if plan_iterations > 0:\n",
    "            return Command(goto=\"reporter\")\n",
    "        else:\n",
    "            return Command(goto=\"__end__\")\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"current_plan\": Plan.model_validate(new_plan),\n",
    "            \"plan_iterations\": plan_iterations,\n",
    "            \"locale\": new_plan[\"locale\"],\n",
    "        },\n",
    "        goto=goto,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf080167-9d3c-49b3-943f-6aba2aaf5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_team_node(state: State):\n",
    "    \"\"\"Research team node that collaborates on tasks.\"\"\"\n",
    "    logger.info(\"Research team is collaborating on tasks.\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17209e-f290-4c23-9f7f-2b1ac16f32c3",
   "metadata": {},
   "source": [
    "## Runtime configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24f0f20b-3286-4c94-99b5-a7f1c66dae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "\n",
    "#used in configuration\n",
    "class ReportStyle(enum.Enum):\n",
    "    ACADEMIC = \"academic\"\n",
    "    POPULAR_SCIENCE = \"popular_science\"\n",
    "    NEWS = \"news\"\n",
    "    SOCIAL_MEDIA = \"social_media\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bc13384-ba8f-4233-9f78-6e6bcefde0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, fields\n",
    "\n",
    "#used in apply_prompt_templates\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields.\"\"\"\n",
    "\n",
    "    resources: list[Resource] = field(\n",
    "        default_factory=list\n",
    "    )  # Resources to be used for the research\n",
    "    max_plan_iterations: int = 1  # Maximum number of plan iterations\n",
    "    max_step_num: int = 3  # Maximum number of steps in a plan\n",
    "    max_search_results: int = 3  # Maximum number of search results\n",
    "    mcp_settings: dict = None  # MCP settings, including dynamic loaded tools\n",
    "    report_style: str = ReportStyle.ACADEMIC.value  # Report style\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2196b55a-03cd-4dfe-b4be-e9a3ec6e60fa",
   "metadata": {},
   "source": [
    "### To combine and access State and Config in the Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ce7e08-86b3-47d6-8ab3-8de6965968cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "def apply_prompt_template(\n",
    "    prompt_name: str, state: AgentState, configurable: Configuration = None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Apply template variables to a prompt template and return formatted messages.\n",
    "\n",
    "    Args:\n",
    "        prompt_name: Name of the prompt template to use\n",
    "        state: Current agent state containing variables to substitute\n",
    "\n",
    "    Returns:\n",
    "        List of messages with the system prompt as the first message\n",
    "    \"\"\"\n",
    "    # Convert state to dict for template rendering\n",
    "    state_vars = {\n",
    "        \"CURRENT_TIME\": datetime.now().strftime(\"%a %b %d %Y %H:%M:%S %z\"),\n",
    "        **state,\n",
    "    }\n",
    "\n",
    "    # Add configurable variables\n",
    "    if configurable:\n",
    "        state_vars.update(dataclasses.asdict(configurable))\n",
    "\n",
    "    try:\n",
    "        template = env.get_template(f\"templates/{prompt_name}.md\")\n",
    "        system_prompt = template.render(**state_vars)\n",
    "        return [{\"role\": \"system\", \"content\": system_prompt}] + state[\"messages\"]\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error applying template {prompt_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "276ffd9b-2d0b-4e00-9fcb-65ba9d67f804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "def create_agent(agent_name: str, agent_type: str, tools: list, prompt_template: str):\n",
    "    \"\"\"Factory function to create agents with consistent configuration.\"\"\"\n",
    "    return create_react_agent(\n",
    "        name=agent_name,\n",
    "        model=basic_llm,\n",
    "        tools=tools,\n",
    "        prompt=lambda state: apply_prompt_template(prompt_template, state),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33d3a98e-dafa-4e8c-bd04-e87500209993",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tools.tavily_search.tavily_search_results_with_images import (\n",
    "    TavilySearchResultsWithImages,\n",
    ")\n",
    "\n",
    "from src.tools.decorators import create_logged_tool\n",
    "\n",
    "LoggedTavilySearch = create_logged_tool(TavilySearchResultsWithImages)\n",
    "\n",
    "from src.tools import (\n",
    "    crawl_tool,\n",
    "get_web_search_tool,\n",
    "    get_retriever_tool,\n",
    "    python_repl_tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "89f79f14-06fa-4d7a-8e25-837594a113d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _execute_agent_step(\n",
    "    state: State, agent, agent_name: str\n",
    ") -> Command[Literal[\"research_team\"]]:\n",
    "    \"\"\"Helper function to execute a step using the specified agent.\"\"\"\n",
    "    current_plan = state.get(\"current_plan\")\n",
    "    observations = state.get(\"observations\", [])\n",
    "\n",
    "    # Find the first unexecuted step\n",
    "    current_step = None\n",
    "    completed_steps = []\n",
    "    for step in current_plan.steps:\n",
    "        if not step.execution_res:\n",
    "            current_step = step\n",
    "            break\n",
    "        else:\n",
    "            completed_steps.append(step)\n",
    "\n",
    "    if not current_step:\n",
    "        logger.warning(\"No unexecuted step found\")\n",
    "        return Command(goto=\"research_team\")\n",
    "\n",
    "    logger.info(f\"Executing step: {current_step.title}, agent: {agent_name}\")\n",
    "\n",
    "    # Format completed steps information\n",
    "    completed_steps_info = \"\"\n",
    "    if completed_steps:\n",
    "        completed_steps_info = \"# Existing Research Findings\\n\\n\"\n",
    "        for i, step in enumerate(completed_steps):\n",
    "            completed_steps_info += f\"## Existing Finding {i + 1}: {step.title}\\n\\n\"\n",
    "            completed_steps_info += f\"<finding>\\n{step.execution_res}\\n</finding>\\n\\n\"\n",
    "\n",
    "    # Prepare the input for the agent with completed steps info\n",
    "    agent_input = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=f\"{completed_steps_info}# Current Task\\n\\n## Title\\n\\n{current_step.title}\\n\\n## Description\\n\\n{current_step.description}\\n\\n## Locale\\n\\n{state.get('locale', 'en-US')}\"\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16ee60-c7f5-448f-9aed-44d76526a9bd",
   "metadata": {},
   "source": [
    "## Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c76f93e9-5a4d-4f97-b3ea-464796427dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Environment, FileSystemLoader, select_autoescape\n",
    "\n",
    "# Initialize Jinja2 environment\n",
    "env = Environment(\n",
    "    loader=FileSystemLoader(os.path.dirname('.')),\n",
    "    autoescape=select_autoescape(),\n",
    "    trim_blocks=True,\n",
    "    lstrip_blocks=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63da82e8-5ce2-43eb-bdf5-523ed1546120",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def handoff_to_planner(\n",
    "    research_topic: Annotated[str, \"The topic of the research task to be handed off.\"],\n",
    "    locale: Annotated[str, \"The user's detected language locale (e.g., en-US, zh-CN).\"],\n",
    "):\n",
    "    \"\"\"Handoff to planner agent to do plan.\"\"\"\n",
    "    # This tool is not returning anything: we're just using it\n",
    "    # as a way for LLM to signal that it needs to hand off to planner agent\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8c7ba91-83dc-4d2f-8f27-5ced5884b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_to_running_research_team(state: State):\n",
    "    current_plan = state.get(\"current_plan\")\n",
    "    if not current_plan or not current_plan.steps:\n",
    "        return \"planner\"\n",
    "    if all(step.execution_res for step in current_plan.steps):\n",
    "        return \"planner\"\n",
    "    for step in current_plan.steps:\n",
    "        if not step.execution_res:\n",
    "            break\n",
    "    if step.step_type and step.step_type == StepType.RESEARCH:\n",
    "        return \"researcher\"\n",
    "    if step.step_type and step.step_type == StepType.PROCESSING:\n",
    "        return \"coder\"\n",
    "    return \"planner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f2a8a852-f3a5-42a6-86d8-e4a3232d3b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "\n",
    "\"\"\"Build and return the base state graph with all nodes and edges.\"\"\"\n",
    "builder = StateGraph(State)\n",
    "builder.add_edge(START, \"coordinator\")\n",
    "builder.add_node(\"coordinator\", coordinator_node)\n",
    "builder.add_node(\"background_investigator\", background_investigation_node)\n",
    "builder.add_node(\"planner\", planner_node)\n",
    "builder.add_node(\"reporter\", reporter_node)\n",
    "builder.add_node(\"research_team\", research_team_node)\n",
    "builder.add_node(\"researcher\", researcher_node)\n",
    "builder.add_node(\"coder\", coder_node)\n",
    "builder.add_node(\"human_feedback\", human_feedback_node)\n",
    "builder.add_edge(\"background_investigator\", \"planner\")\n",
    "builder.add_conditional_edges(\n",
    "    \"research_team\",\n",
    "    continue_to_running_research_team,\n",
    "    [\"planner\", \"researcher\", \"coder\"],\n",
    ")\n",
    "builder.add_edge(\"reporter\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f736c6a3-cf94-4994-9e10-b8170dfa4935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div class=\"mermaid-c6c4583b-5b32-44a0-95bd-d354801bbcb6\"></div>\n",
       "        <script type=\"module\">\n",
       "            import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10.1.0/+esm'\n",
       "            const graphDefinition = '%%{init: {\"flowchart\": {\"curve\": \"linear\"}}}%%\\ngraph TD;\\n\t__start__([<p>__start__</p>]):::first\\n\tcoordinator(coordinator)\\n\tbackground_investigator(background_investigator)\\n\tplanner(planner)\\n\treporter(reporter)\\n\tresearch_team(research_team)\\n\tresearcher(researcher)\\n\tcoder(coder)\\n\thuman_feedback(human_feedback)\\n\t__end__([<p>__end__</p>]):::last\\n\t__start__ --> coordinator;\\n\tbackground_investigator --> planner;\\n\tcoder -.-> research_team;\\n\tcoordinator -.-> __end__;\\n\tcoordinator -.-> background_investigator;\\n\tcoordinator -.-> planner;\\n\thuman_feedback -.-> __end__;\\n\thuman_feedback -.-> planner;\\n\thuman_feedback -.-> reporter;\\n\thuman_feedback -.-> research_team;\\n\tplanner -.-> human_feedback;\\n\tplanner -.-> reporter;\\n\tresearch_team -.-> coder;\\n\tresearch_team -.-> planner;\\n\tresearch_team -.-> researcher;\\n\tresearcher -.-> research_team;\\n\treporter --> __end__;\\n\tclassDef default fill:#f2f0ff,line-height:1.2\\n\tclassDef first fill-opacity:0\\n\tclassDef last fill:#bfb6fc\\n';\n",
       "            const element = document.querySelector('.mermaid-c6c4583b-5b32-44a0-95bd-d354801bbcb6');\n",
       "            const { svg } = await mermaid.render('graphDiv-c6c4583b-5b32-44a0-95bd-d354801bbcb6', graphDefinition);\n",
       "            element.innerHTML = svg;\n",
       "        </script>\n",
       "        "
      ],
      "text/plain": [
       "<mermaid.mermaid.Mermaid at 0x107fdc6b0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Mermaid(graph.get_graph(xray=True).draw_mermaid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f771e1c-a9d0-4058-8abb-3b65becdca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"node execution\")\n",
    "from datetime import datetime\n",
    "import dataclasses\n",
    "\n",
    "question = \"\"\"What is the outlook for the cloud computing industry both in the west and emerging markets or regions?\n",
    "What are some of the drivers of its growth and possible challenges or constraints?  What does the competitive landscape look like for leading\n",
    "providers of cloud services?\"\"\"\n",
    "\n",
    "initial_state = {\n",
    "        #Setting up State\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": question}],\n",
    "        \"auto_accepted_plan\": True,\n",
    "        \"enable_background_investigation\": True,\n",
    "    }\n",
    "\n",
    "config = {\n",
    "    #runtime configuration or variables\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"default\",\n",
    "            \"max_plan_iterations\": 1,\n",
    "            \"max_step_num\": 3,\n",
    "            \"mcp_settings\": {\n",
    "                \"servers\": {\n",
    "                    \"mcp-github-trending\": {\n",
    "                        \"transport\": \"stdio\",\n",
    "                        \"command\": \"uvx\",\n",
    "                        \"args\": [\"mcp-github-trending\"],\n",
    "                        \"enabled_tools\": [\"get_github_trending_repositories\"],\n",
    "                        \"add_to_agents\": [\"researcher\"],\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "        \"recursion_limit\": 100,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "060b3038-1c41-46c0-a2d0-13fdb294fab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is the outlook for the cloud computing industry both in the west and emerging markets or regions?\n",
      "What are some of the drivers of its growth and possible challenges or constraints?  What does the competitive landscape look like for leading\n",
      "providers of cloud services?\n",
      "sync [\n",
      "  {\n",
      "    \"type\": \"page\",\n",
      "    \"title\": \"Cloud Computing Market Size, Trends, Growth Drivers - 2035\",\n",
      "    \"url\": \"https://www.marketresearchfuture.com/reports/cloud-computing-market-1013\",\n",
      "    \"content\": \"**Cloud Computing Market Overview**\\n-----------------------------------\\n\\nAs per MRFR analysis, the Cloud Computing Market Size was estimated at 293.4 (USD Billion) in 2023.The Cloud Computing Market Industry is expected to grow from 318.98(USD Billion) in 2024 to 800 (USD Billion) by 2035. The Cloud Computing Market CAGR (growth rate) is expected to be around 8.72% during the forecast period (2025 - 2035). [...] *   \\n\\nSmall and Medium Enterprises\\n\\n    *   \\n\\nLarge Enterprises\\n\\n    *   \\n\\nGovernment\\n\\n### **Cloud Computing Market Application Outlook**\\n\\n    *   \\n\\nData Backup and Recovery\\n\\n    *   \\n\\nApplication Hosting\\n\\n    *   \\n\\nDisaster Recovery\\n\\n    *   \\n\\nBig Data Analytics\\n\\n### **Cloud Computing Market Regional Outlook**\\n\\n    *   \\n\\nNorth America\\n\\n    *   \\n\\nEurope\\n\\n    *   \\n\\nSouth America\\n\\n    *   \\n\\nAsia Pacific\\n\\n    *   \\n\\nMiddle East and Africa [...] Recent developments in the Global Cloud Computing Market reflect a dynamic and rapidly evolving landscape. Within the market, companies like Amazon Web Services and Microsoft are continuously expanding their service offerings, while Alibaba Cloud and Google are focusing on enhancing infrastructure to support increased digital demands. Notably, in October 2023, Salesforce and Cloudflare announced a strategic partnership aimed at integrating their platforms to enhance security and performance for\",\n",
      "    \"score\": 0.638588\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"page\",\n",
      "    \"title\": \"United States Top Cloud Computing Service Market Size, Growth ...\",\n",
      "    \"url\": \"https://www.linkedin.com/pulse/united-states-top-cloud-computing-service-market-size-smome/\",\n",
      "    \"content\": \"The United States Top Cloud Computing Service Market: Regional Dynamics and Forecast Insights provides an in-depth examination of market performance across key U.S. regions, including the Northeast, Midwest, South, and West. This analysis highlights regional variations in demand, consumer behavior, competitive landscape, and regulatory influences that shape the Top Cloud Computing Service industry. By exploring localized trends and growth drivers, the report offers valuable foresight into how\",\n",
      "    \"score\": 0.6208291\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"page\",\n",
      "    \"title\": \"Cloud Computing Market to Reach USD 2321.1 Billion by 2032\",\n",
      "    \"url\": \"https://www.globenewswire.com/news-release/2023/03/17/2629610/0/en/Cloud-Computing-Market-to-Reach-USD-2-321-1-Billion-by-2032-Exploring-the-Diverse-Applications-of-Cloud-Computing.html\",\n",
      "    \"content\": \"New York, March 17, 2023 (GLOBE NEWSWIRE) -- The global [cloud computing market](https://market.us/report/cloud-computing-market/ \\\"cloud computing market\\\") was valued at around **USD 546.1 billion in 2022** and is estimated to be worth approximately USD **2,321 billion in 2032**, growing at a **CAGR of 16%** between 2023 and 2032. Cloud computing is a network or model where programs or applications can run that can be accessed from many devices or servers simultaneously. Emerging technologies [...] Asia Pacific will be the fastest-growing region over the forecast period. The region’s market growth has been boosted by the rapid rise of India and China. The emergence of regional players such as Alibaba Group has also contributed to the region's progress.\\n\\n**Competitive Landscape**\",\n",
      "    \"score\": 0.6052631\n",
      "  }\n",
      "]\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: planner\n",
      "\n",
      "{\n",
      "    \"locale\": \"en-US\",\n",
      "    \"has_enough_context\": false,\n",
      "    \"thought\": \"The user wants a detailed outlook on the cloud computing industry covering both Western and emerging markets, including growth drivers, challenges, and competitive landscape of leading providers. The background data provides a good starting point but is not fully comprehensive or recent enough to answer all aspects of the query in detail. More detailed, region-specific, and multi-perspective data is needed to meet information quantity and quality standards. Therefore, a focused three-step plan is required to gather extensive and deep insights on industry outlook, growth drivers and challenges, and competitive landscape.\",\n",
      "    \"title\": \"Research Plan for Comprehensive Outlook on the Cloud Computing Industry in Western and Emerging Markets\",\n",
      "    \"steps\": [\n",
      "        {\n",
      "            \"need_search\": true,\n",
      "            \"title\": \"Comprehensive Market Outlook and Growth Drivers in Western and Emerging Markets\",\n",
      "            \"description\": \"Collect detailed and up-to-date data on the current and forecasted size of the cloud computing market in major Western regions (e.g., North America, Europe) and emerging markets (e.g., Asia Pacific, Latin America, Middle East & Africa). Include CAGR, revenue projections, adoption rates, and market segmentation by industry and application. Identify key drivers fueling growth such as digital transformation, infrastructure development, technology adoption, and regulatory factors for each region. Include analysis of emerging regional players and technological trends influencing market growth.\",\n",
      "            \"step_type\": \"research\"\n",
      "        },\n",
      "        {\n",
      "            \"need_search\": true,\n",
      "            \"title\": \"Challenges, Constraints, and Risk Factors Impacting Cloud Computing Industry Growth\",\n",
      "            \"description\": \"Gather extensive information on the major challenges cloud computing faces globally and regionally, including security and privacy concerns, regulatory compliance, technological barriers, infrastructure limitations, cost factors, and geopolitical risks. Collect data on market constraints impacting adoption in emerging versus established markets. Explore alternative perspectives and case studies about risks and mitigations employed by cloud providers and users.\",\n",
      "            \"step_type\": \"research\"\n",
      "        },\n",
      "        {\n",
      "            \"need_search\": true,\n",
      "            \"title\": \"Competitive Landscape and Strategic Positioning of Leading Cloud Service Providers\",\n",
      "            \"description\": \"Assemble detailed competitive analysis of leading cloud providers like Amazon Web Services, Microsoft Azure, Google Cloud, Alibaba Cloud, and others. Include market share data, service portfolio comparisons, recent strategic partnerships, regional strengths, and investment trends. Examine competitive dynamics in Western versus emerging markets, highlighting how providers tailor offerings or expand infrastructure to dominate different regions. Include data on recent mergers, acquisitions, and innovation initiatives shaping industry competition.\",\n",
      "            \"step_type\": \"research\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "last_message_cnt = 0\n",
    "async for s in graph.astream(\n",
    "    input=initial_state, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    try:\n",
    "        if isinstance(s, dict) and \"messages\" in s:\n",
    "            if len(s[\"messages\"]) <= last_message_cnt:\n",
    "                continue\n",
    "            last_message_cnt = len(s[\"messages\"])\n",
    "            message = s[\"messages\"][-1]\n",
    "            if isinstance(message, tuple):\n",
    "                print(message)\n",
    "            else:\n",
    "                message.pretty_print()\n",
    "        else:\n",
    "            # For any other output format\n",
    "            print(f\"Output: {s}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing stream output: {e}\")\n",
    "        print(f\"Error processing output: {str(e)}\")\n",
    "\n",
    "logger.info(\"Async workflow completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8177bb-2b62-4357-ada3-3a8d41d08bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bde80ead-29d2-4741-87a0-c932d597dbc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (574571583.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m\"\"\"\"last_message_cnt = 0\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"last_message_cnt = 0\n",
    "results = []\n",
    "async for s in graph.astream(\n",
    "    input=initial_state, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    try:\n",
    "        if isinstance(s, dict) and \"messages\" in s:\n",
    "            if len(s[\"messages\"]) <= last_message_cnt:\n",
    "                continue\n",
    "            last_message_cnt = len(s[\"messages\"])\n",
    "            message = s[\"messages\"][-1]\n",
    "            if isinstance(message, tuple):\n",
    "                results.append(message)\n",
    "        else:\n",
    "            # For any other output format\n",
    "            print(f\"Output: {s}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing stream output: {e}\")\n",
    "        print(f\"Error processing output: {str(e)}\")\n",
    "\n",
    "logger.info(\"Async workflow completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e05bbd3-b918-4f42-bfda-088e127e901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d89f43-7c71-4e23-9f13-6a135bdf0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "events[1]['messages'][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988706af-4bf9-4ce6-b517-9a685bb1a91d",
   "metadata": {},
   "source": [
    "### Examining the state of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16284d51-6325-432a-a18c-c43a77066d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "{k for k, v in snapshot.values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ff8f0-3fe3-4d98-aea3-0f5d2fa73bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshot = graph.get_state(config)\n",
    "{k:v for k, v in snapshot.values.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e4dbe84-f926-4ad9-9540-8ede8c6dcc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Messages:  2 Next:  ()\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('researcher',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('research_team',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  2 Next:  ('human_feedback',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  1 Next:  ('planner',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  1 Next:  ('background_investigator',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  1 Next:  ('coordinator',)\n",
      "--------------------------------------------------------------------------------\n",
      "Num Messages:  0 Next:  ('__start__',)\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#graph steps and message collection\n",
    "for state in graph.get_state_history(config):\n",
    "    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a02bd5-6526-4ed7-88d6-610461a50d4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
